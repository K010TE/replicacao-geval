
# ðŸ“ ReplicaÃ§Ã£o do G-EVAL: AvaliaÃ§Ã£o de GeraÃ§Ã£o de Linguagem Natural com GPT-4

Este repositÃ³rio contÃ©m a **replicaÃ§Ã£o do experimento descrito no artigo [G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634)**, com o objetivo de avaliar a capacidade de modelos de linguagem em julgar automaticamente a qualidade de saÃ­das de sistemas de GeraÃ§Ã£o de Linguagem Natural (NLG), com base em critÃ©rios como **coerÃªncia**, **consistÃªncia**, **fluÃªncia** e **relevÃ¢ncia**.

## ðŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ replicacao.ipynb         # Notebook principal com toda a execuÃ§Ã£o
â”œâ”€â”€ avaliar_correlacoes.py   # Script para calcular correlaÃ§Ãµes com notas humanas
â”œâ”€â”€ results/                 # Arquivos .json com saÃ­das do G-EVAL
â”œâ”€â”€ .env                     # Arquivo com a chave da OpenAI (NÃƒO subir no GitHub)
â”œâ”€â”€ requirements.txt         # DependÃªncias do projeto
â””â”€â”€ README.md                # Este arquivo
```

## âš™ï¸ Requisitos

VocÃª precisarÃ¡ ter o **Python 3.8 ou superior** instalado. Recomenda-se o uso de um ambiente virtual.

Instale as dependÃªncias com:

```bash
pip install -r requirements.txt
```

## ðŸ” ConfiguraÃ§Ã£o da Chave da OpenAI

Este projeto utiliza a API do GPT-4 via OpenAI. Para manter sua chave segura:

1. Crie um arquivo `.env` na raiz do projeto:
    ```ini
    OPENAI_API_KEY=sk-proj-sua-chave-aqui
    ```
2. **Nunca suba esse arquivo para o GitHub** (o `.gitignore` jÃ¡ estÃ¡ configurado para ignorÃ¡-lo).

## â–¶ï¸ Como Executar

A replicaÃ§Ã£o pode ser feita diretamente pelo notebook:

1. Abra o arquivo `replicacao.ipynb`.
2. Execute as cÃ©lulas sequencialmente.
3. Os resultados de avaliaÃ§Ã£o serÃ£o salvos e plotados ao final.

TambÃ©m Ã© possÃ­vel rodar a avaliaÃ§Ã£o estatÃ­stica separadamente:

```bash
python avaliar_correlacoes.py
```

## ðŸ“Š Resultados

Os resultados obtidos na replicaÃ§Ã£o incluem os coeficientes de correlaÃ§Ã£o entre as notas atribuÃ­das pelo G-EVAL (via GPT-4) e as notas humanas de referÃªncia, como **Pearson**, **Spearman** e **Kendall-Tau** para cada critÃ©rio avaliado.

## ðŸ“š ReferÃªncia Original

> Liu, X., Zhu, C., Mou, L., & Yang, D. (2023).  
> **G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment**.  
> *arXiv preprint arXiv:2303.16634*.  
> [https://arxiv.org/abs/2303.16634](https://arxiv.org/abs/2303.16634)
